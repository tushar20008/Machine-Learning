{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4487 - Tutorial 10\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "In this tutorial you will use stochastic gradient descent to train classifiers quickly.\n",
    "\n",
    "First we need to initialize Python.  Run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "import glob\n",
    "import os\n",
    "import IPython.utils.warn as warn\n",
    "import cPickle, gzip, numpy\n",
    "import time\n",
    "\n",
    "random.seed(100)\n",
    "rbow = plt.get_cmap('rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a larger version of the MNIST digits dataset.  Download \"mnist.pkl.gz\" from Canvas and put it in the same directory as this ipynb file. The training set has 50,000 images and the test set has 10,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "trainX,trainY = train_set\n",
    "valX,valY = valid_set\n",
    "testX,testY = test_set\n",
    "\n",
    "print trainX.shape\n",
    "print testX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train a linear SVM using the standard algorithm, and time how long it takes.  Run the below code.  It may take a few minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time (sec): 99.210545\n"
     ]
    }
   ],
   "source": [
    "starttime = time.clock()\n",
    "clfo = svm.LinearSVC(C=1.)\n",
    "clfo.fit(trainX, trainY)\n",
    "print \"elapsed time (sec):\", time.clock() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the training and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracies: 0.92626 0.9158\n"
     ]
    }
   ],
   "source": [
    "Ypred = clfo.predict(trainX)\n",
    "trainacc_svm = metrics.accuracy_score(Ypred, trainY)\n",
    "\n",
    "Ypred = clfo.predict(testX)\n",
    "testacc_svm = metrics.accuracy_score(Ypred, testY)\n",
    "print \"SVM accuracies:\", trainacc_svm, testacc_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "Now train a SGD classifier using the SVM loss and L2 penalty.  Time the amount of time it takes to fit the classifier (use the `fit` function).  Calculate the training and test error of the SGD classifier.  Use `alpha=0.1`.  Remember, alpha = 1/C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time (sec): 5.322864\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(\n",
    "    loss='hinge',  # SVM loss (change to 'log' for logistic regression)\n",
    "    penalty='l2',  # standard penalty (change to 'l1' for feature selection)\n",
    "    alpha=0.1,     # penalty parameter: C=1/alpha \n",
    "    n_iter=5,\n",
    "    average=True)  # use a running average for classifier weights\n",
    "                   #   makes classifier more stable between batches\n",
    "\n",
    "starttime = time.clock()\n",
    "clf.fit(trainX, trainY)\n",
    "print \"elapsed time (sec):\", time.clock() - starttime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86476 0.8754\n"
     ]
    }
   ],
   "source": [
    "Ypred = clf.predict(trainX)\n",
    "trainacc_sgd = metrics.accuracy_score(Ypred, trainY)\n",
    "\n",
    "Ypred = clf.predict(testX)\n",
    "testacc_sgd = metrics.accuracy_score(Ypred, testY)\n",
    "print trainacc_sgd, testacc_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How does the speed and the accuracy compare with the original SVM?_\n",
    "- **INSERT YOUR ANSWER HERE**\n",
    "- faster, but about 5% decrease in accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel SGD Classifier\n",
    "Now train a parallel SGD classifier using IPython clusters, and measure the fitting time.  Use the same value for alpha as your SGD Classifier. Try different batch sizes (B) and number of processes (K).  Calculate the training and test error.\n",
    "\n",
    "First start the IP clusters using the \"IPython Clusters\" tab in Jupyter.  If the tab says \"Clusters tab now provided by IPython parallel\", then run `ipcluster nbextension enable` to enable it.  Alternatively, you can run  `ipcluster start -n 4` on the command line to directly start 4 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# load the client interface\n",
    "import ipyparallel\n",
    "\n",
    "clients = ipyparallel.Client()\n",
    "clients.block = True   # wait for calculations to finish\n",
    "print clients.ids      # client process ids\n",
    "\n",
    "# get the load-balanced scheduler\n",
    "lview = clients.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# load libraries on all clients\n",
    "from numpy import *\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def par_sgd(data, param):\n",
    "    # run SGD on a dataset\n",
    "    clf = linear_model.SGDClassifier(\n",
    "        loss='hinge', \n",
    "        penalty='l2',\n",
    "        alpha=param['alpha'],\n",
    "        average=False)  # don't use averaging, since we will do it later\n",
    "    clf.fit(data['trainX'], data['trainY'])\n",
    "    return clf\n",
    "\n",
    "def combine_sgd(clfs):\n",
    "    # combine sgd classifiers\n",
    "    \n",
    "    # make a copy of the first one\n",
    "    import copy\n",
    "    clfout = copy.deepcopy(clfs[0])\n",
    "    K = len(clfs)\n",
    "\n",
    "    # add all the remaining ones to it\n",
    "    for i in range(1,K):\n",
    "        clfout.coef_ += clfs[i].coef_\n",
    "        clfout.intercept_ += clfs[i].intercept_\n",
    "\n",
    "    # take the average\n",
    "    clfout.coef_ /= K\n",
    "    clfout.intercept_ /= K\n",
    "\n",
    "    return clfout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time (sec): 0.312453\n",
      "elapsed time (sec): 0.836902\n",
      "psgd accuracy: 0.89064 0.8984\n"
     ]
    }
   ],
   "source": [
    "param = {'alpha': 0.01}\n",
    "\n",
    "K = 20           # use 10 processes\n",
    "N = len(trainX)  # dataset size\n",
    "B = int(0.05*N)  # batch size\n",
    "\n",
    "random.seed(612)\n",
    "\n",
    "# split data into batches\n",
    "starttime = time.clock()\n",
    "data_batches = []\n",
    "for i in range(K):\n",
    "    rp = random.permutation(N)\n",
    "    trainX_shuffle = trainX[rp[range(B)]]\n",
    "    trainY_shuffle = trainY[rp[range(B)]]\n",
    "    data_batches.append({'trainX': trainX_shuffle, 'trainY': trainY_shuffle})\n",
    "print \"elapsed time (sec):\", time.clock() - starttime\n",
    "\n",
    "# run par_sgd on each batch of data\n",
    "lview.block = True\n",
    "starttime = time.clock()\n",
    "clfs = lview.map(par_sgd, data_batches, [param]*K)\n",
    "clf = combine_sgd(clfs)  \n",
    "print \"elapsed time (sec):\", time.clock() - starttime\n",
    "\n",
    "# without load-balanced view (for testing)\n",
    "#clfs = map(par_sgd, data_batches, [param]*K)\n",
    "\n",
    "# combine classifiers\n",
    "\n",
    "# training error\n",
    "Ypred = clf.predict(trainX)\n",
    "trainacc_psgd = metrics.accuracy_score(Ypred, trainY)\n",
    "\n",
    "Ypred = clf.predict(testX)\n",
    "testacc_psgd = metrics.accuracy_score(Ypred, testY)\n",
    "\n",
    "print \"psgd accuracy:\", trainacc_psgd, testacc_psgd\n",
    "\n",
    "# compare with individual classifiers\n",
    "#errs = []\n",
    "#for myclf in clfs:\n",
    "#    mypred = myclf.predict(trainX)\n",
    "#    errs.append( mean(mypred != trainY) )\n",
    "#\n",
    "#    \n",
    "#print \"individual clf errors:\", errs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How does the speed and the accuracy compare with the original SVM?_\n",
    "- **INSERT YOUR ANSWER HERE**\n",
    "- faster, and about 3% decrease in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
